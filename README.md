# Plateforme d'Ã©valuation de performance des moteurs SPARQL

Une application Streamlit complÃ¨te pour l'Ã©valuation comparative des performances des moteurs SPARQL, spÃ©cialement conÃ§ue pour comparer Virtuoso et Jena Fuseki.

## ğŸš€ FonctionnalitÃ©s principales

- **Tests de performance automatisÃ©s** : ExÃ©cution automatique de requÃªtes SPARQL avec mesures dÃ©taillÃ©es
- **Comparaison multi-moteurs** : Support pour Virtuoso, Jena Fuseki et autres endpoints SPARQL
- **MÃ©triques complÃ¨tes** : Temps d'exÃ©cution, utilisation CPU/mÃ©moire, taux de succÃ¨s
- **Visualisations interactives** : Graphiques dynamiques avec Plotly
- **Catalogues de requÃªtes** : RequÃªtes prÃ©-dÃ©finies pour LUBM, DBpedia et jeux de donnÃ©es gÃ©nÃ©riques
- **Export des rÃ©sultats** : Export vers CSV, Excel, JSON avec gÃ©nÃ©ration de rapports
- **Interface intuitive** : Interface web moderne avec Streamlit

## ğŸ“ Structure du projet

```
sparql_performance_platform/
â”œâ”€â”€ main.py                          # Point d'entrÃ©e principal
â”œâ”€â”€ config/                          # Configuration
â”‚   â”œâ”€â”€ settings.py                  # ParamÃ¨tres globaux
â”‚   â””â”€â”€ endpoints.py                 # Configuration endpoints
â”œâ”€â”€ core/                            # Logique mÃ©tier
â”‚   â”œâ”€â”€ tester.py                    # Testeur de performance
â”‚   â”œâ”€â”€ executor.py                  # ExÃ©cuteur de requÃªtes
â”‚   â””â”€â”€ metrics.py                   # Collecteur de mÃ©triques
â”œâ”€â”€ queries/                         # Catalogues de requÃªtes
â”‚   â”œâ”€â”€ catalog.py                   # Catalogue principal
â”‚   â”œâ”€â”€ lubm_queries.py              # RequÃªtes LUBM
â”‚   â”œâ”€â”€ dbpedia_queries.py           # RequÃªtes DBpedia
â”‚   â””â”€â”€ generic_queries.py           # RequÃªtes gÃ©nÃ©riques
â”œâ”€â”€ visualization/                   # Visualisations
â”‚   â”œâ”€â”€ visualizer.py                # Visualiseur principal
â”‚   â”œâ”€â”€ charts.py                    # Graphiques spÃ©cialisÃ©s
â”‚   â””â”€â”€ reports.py                   # GÃ©nÃ©ration rapports
â”œâ”€â”€ ui/                              # Interface utilisateur
â”‚   â”œâ”€â”€ sidebar.py                   # Barre latÃ©rale
â”‚   â””â”€â”€ tabs/                        # Onglets de l'interface
â”œâ”€â”€ utils/                           # Utilitaires
â”‚   â”œâ”€â”€ data_manager.py              # Gestion des donnÃ©es
â”‚   â”œâ”€â”€ export_manager.py            # Gestion exports
â”‚   â””â”€â”€ helpers.py                   # Fonctions utilitaires
â””â”€â”€ tests/                           # Tests unitaires
```

## ğŸ› ï¸ Installation

### PrÃ©requis

- Python 3.8+
- Moteurs SPARQL (Virtuoso et/ou Jena Fuseki) installÃ©s et configurÃ©s

### Installation des dÃ©pendances

```bash
# Cloner le repository
git clone <repository-url>
cd sparql_performance_platform

# Installer les dÃ©pendances
pip install -r requirements.txt
```

### Configuration des moteurs SPARQL

#### Virtuoso

```bash
# Installation via Docker (recommandÃ©)
docker run -d \
  --name virtuoso \
  -p 8890:8890 \
  -e DBA_PASSWORD=dba \
  tenforce/virtuoso:1.3.2-virtuoso7.2.5
```

#### Jena Fuseki

```bash
# Installation via Docker
docker run -d \
  --name fuseki \
  -p 3030:3030 \
  stain/jena-fuseki:latest
```

## ğŸš€ Utilisation

### Lancement de l'application

```bash
streamlit run main.py
```

L'application sera accessible Ã  l'adresse : `http://localhost:8501`

### Configuration initiale

1. **Endpoints SPARQL** : Configurez les URLs de vos endpoints dans la barre latÃ©rale
2. **Jeu de donnÃ©es** : SÃ©lectionnez le type de jeu de donnÃ©es (LUBM, DBpedia, etc.)
3. **ParamÃ¨tres de test** : Ajustez le nombre d'itÃ©rations, niveau de concurrence, etc.

### ExÃ©cution des tests

1. **SÃ©lection des requÃªtes** : Choisissez les types de requÃªtes Ã  tester
2. **Lancement** : Cliquez sur "ExÃ©cuter les tests"
3. **Analyse** : Explorez les rÃ©sultats dans les onglets RÃ©sultats et Visualisation
4. **Export** : TÃ©lÃ©chargez les rÃ©sultats ou gÃ©nÃ©rez un rapport

## ğŸ“Š Types de tests supportÃ©s

### Par complexitÃ©

- **RequÃªtes simples** : Pattern matching basique
- **RequÃªtes de jointure** : Jointures entre entitÃ©s
- **RequÃªtes d'agrÃ©gation** : GROUP BY, COUNT, SUM, etc.
- **RequÃªtes avec filtres** : Conditions FILTER
- **RequÃªtes avancÃ©es** : OPTIONAL, UNION, MINUS
- **Sous-requÃªtes** : RequÃªtes imbriquÃ©es complexes

### Par jeu de donnÃ©es

- **LUBM** : Lehigh University Benchmark (domaine universitaire)
- **DBpedia** : DonnÃ©es encyclopÃ©diques structurÃ©es
- **BSBM** : Berlin SPARQL Benchmark (e-commerce)
- **YAGO** : Base de connaissances gÃ©ographique
- **PersonnalisÃ©** : Vos propres jeux de donnÃ©es RDF

## ğŸ“ˆ MÃ©triques collectÃ©es

### Performance

- **Temps d'exÃ©cution** : DurÃ©e totale de chaque requÃªte
- **DÃ©bit** : RequÃªtes par seconde
- **Latence** : Temps de premiÃ¨re rÃ©ponse

### Ressources systÃ¨me

- **CPU** : Utilisation processeur pendant l'exÃ©cution
- **MÃ©moire** : Consommation RAM
- **RÃ©seau** : Trafic gÃ©nÃ©rÃ© (optionnel)

### FiabilitÃ©

- **Taux de succÃ¨s** : Pourcentage de requÃªtes rÃ©ussies
- **Gestion d'erreurs** : Classification des erreurs
- **StabilitÃ©** : Variation des performances

## ğŸ¨ Visualisations disponibles

- **Graphiques en barres** : Comparaison des temps d'exÃ©cution
- **Graphiques de dispersion** : CorrÃ©lation entre moteurs
- **Heatmaps** : Vue d'ensemble des performances
- **Graphiques de tendance** : Ã‰volution par itÃ©ration
- **Tableaux rÃ©capitulatifs** : Statistiques dÃ©taillÃ©es

## ğŸ“¤ Export et rapports

### Formats d'export

- **CSV** : DonnÃ©es brutes pour analyse externe
- **Excel** : Feuilles multiples avec rÃ©sumÃ©s
- **JSON** : DonnÃ©es structurÃ©es avec mÃ©tadonnÃ©es

### Types de rapports

- **Rapport complet** : Analyse dÃ©taillÃ©e de tous les aspects
- **RÃ©sumÃ© exÃ©cutif** : Vue d'ensemble pour les dÃ©cideurs
- **Rapport technique** : DÃ©tails d'implÃ©mentation
- **Comparaison moteurs** : Focus sur les diffÃ©rences

## ğŸ”§ Configuration avancÃ©e

### Profils de test prÃ©dÃ©finis

```python
# Test rapide (dÃ©veloppement)
{
    "num_iterations": 3,
    "warmup_iterations": 1,
    "concurrent_queries": 1,
    "query_types": ["simple"]
}

# Test complet (production)
{
    "num_iterations": 10,
    "warmup_iterations": 3,
    "concurrent_queries": 1,
    "query_types": ["all"]
}

# Test de stress
{
    "num_iterations": 20,
    "warmup_iterations": 5,
    "concurrent_queries": 5,
    "query_types": ["all"]
}
```

### Variables d'environnement

```bash
# Configuration des endpoints par dÃ©faut
export VIRTUOSO_ENDPOINT="http://localhost:8890/sparql"
export FUSEKI_ENDPOINT="http://localhost:3030/dataset/query"

# Configuration des timeouts
export QUERY_TIMEOUT=60
export CONNECTIVITY_TIMEOUT=5

# Mode debug
export DEBUG_MODE=true
```

## ğŸ§ª DÃ©veloppement

### Structure modulaire

Le projet suit une architecture modulaire pour faciliter :

- **MaintenabilitÃ©** : Code organisÃ© par responsabilitÃ©
- **RÃ©utilisabilitÃ©** : Composants indÃ©pendants
- **ExtensibilitÃ©** : Ajout facile de nouveaux moteurs/mÃ©triques
- **TestabilitÃ©** : Tests unitaires par module

### Ajout de nouveaux moteurs

```python
# Dans core/executor.py
class CustomEngineExecutor(QueryExecutor):
    def setup_endpoint(self, endpoint_url, query):
        # ImplÃ©mentation spÃ©cifique au moteur
        pass
```

### Ajout de nouvelles mÃ©triques

```python
# Dans core/metrics.py
class CustomMetricsCollector(MetricsCollector):
    def collect_custom_metrics(self):
        # Collecte de mÃ©triques spÃ©cialisÃ©es
        pass
```

### Tests unitaires

```bash
# ExÃ©cution des tests
python -m pytest tests/

# Tests avec couverture
python -m pytest tests/ --cov=. --cov-report=html
```

## ğŸ“š API et extensibilitÃ©

### Interface de programmation

```python
from core.tester import SPARQLPerformanceTester
from queries.catalog import SPARQLQueryCatalog

# Utilisation programmatique
tester = SPARQLPerformanceTester(
    virtuoso_endpoint="http://localhost:8890/sparql",
    fuseki_endpoint="http://localhost:3030/dataset/query"
)

catalog = SPARQLQueryCatalog()
queries = catalog.get_queries_by_type("LUBM")

results = tester.run_benchmark("test_query", query, 5, 2)
```

### Hooks et callbacks

```python
# Callback de progression
def progress_callback(current, total, message):
    print(f"Progress: {current}/{total} - {message}")

tester.set_progress_callback(progress_callback)
```

## ğŸ” DÃ©pannage

### ProblÃ¨mes courants

#### Endpoint non accessible

```bash
# VÃ©rification de connectivitÃ©
curl -X GET "http://localhost:8890/sparql?query=SELECT%20?s%20WHERE%20{%20?s%20?p%20?o%20}%20LIMIT%201"
```

#### Erreurs de mÃ©moire

- RÃ©duire le nombre d'itÃ©rations simultanÃ©es
- Augmenter la mÃ©moire allouÃ©e aux moteurs SPARQL
- Utiliser des requÃªtes avec LIMIT

#### Performance dÃ©gradÃ©e

- VÃ©rifier la charge systÃ¨me
- RedÃ©marrer les moteurs SPARQL
- Utiliser les itÃ©rations d'Ã©chauffement

### Logs et debug

```python
# Activation du mode debug
import logging
logging.basicConfig(level=logging.DEBUG)
```

## ğŸ“„ Licence

Ce projet est dÃ©veloppÃ© dans le cadre d'un mÃ©moire de Master 2 en Informatique - GÃ©nie Logiciel.

## ğŸ¤ Contribution

### Guide de contribution

1. Fork du projet
2. CrÃ©ation d'une branche feature (`git checkout -b feature/AmazingFeature`)
3. Commit des changements (`git commit -m 'Add AmazingFeature'`)
4. Push vers la branche (`git push origin feature/AmazingFeature`)
5. Ouverture d'une Pull Request

### Standards de code

- **Formatage** : Black pour le formatage automatique
- **Linting** : Flake8 pour la vÃ©rification du style
- **Type hints** : MyPy pour la vÃ©rification des types
- **Documentation** : Docstrings Google style

## ğŸ“ Support

Pour toute question ou problÃ¨me :

- CrÃ©ez une issue sur GitHub
- Consultez la documentation dans le dossier `docs/`
- RÃ©fÃ©rez-vous aux exemples dans `examples/`

## ğŸ”® Roadmap

### Version 2.0

- [ ] Support pour GraphDB et autres moteurs
- [ ] Interface REST API
- [ ] Tableau de bord en temps rÃ©el
- [ ] IntÃ©gration CI/CD pour tests automatiques

### Version 2.1

- [ ] Machine learning pour prÃ©diction de performance
- [ ] Optimisation automatique de requÃªtes
- [ ] Support pour SPARQL 1.1 Update
- [ ] Clustering et distribution des tests

---

**DÃ©veloppÃ© avec â¤ï¸ pour l'Ã©valuation des performances SPARQL**
