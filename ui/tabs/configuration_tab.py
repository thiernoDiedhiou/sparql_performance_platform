"""
Onglet de configuration et ex√©cution des tests
"""

import streamlit as st
import pandas as pd
from typing import Dict, Any
from core.tester import SPARQLPerformanceTester
from queries.catalog import SPARQLQueryCatalog
from ui.components.connectivity_checker import ConnectivityChecker
from ui.components.system_info import SystemInfoDisplay
from utils.data_manager import save_test_results
from utils.helpers import log_message, filter_queries_by_selection

def render_configuration_tab(sidebar_config: Dict[str, Any]):
    """
    Affiche l'onglet de configuration et tests
    
    Args:
        sidebar_config: Configuration de la barre lat√©rale
    """
    st.header("Configuration et ex√©cution des tests")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.subheader("V√©rification de la connectivit√©")
        
        # Composant de v√©rification de connectivit√©
        connectivity_checker = ConnectivityChecker()
        
        if st.button("Tester la connectivit√©"):
            with st.spinner("Test de connectivit√© en cours..."):
                virtuoso_status = connectivity_checker.test_endpoint(
                    sidebar_config["virtuoso_endpoint"], "Virtuoso"
                )
                fuseki_status = connectivity_checker.test_endpoint(
                    sidebar_config["fuseki_endpoint"], "Jena Fuseki"
                )
                
                st.write(f"**Virtuoso:** {virtuoso_status['message']}")
                st.write(f"**Jena Fuseki:** {fuseki_status['message']}")
                
                if virtuoso_status['status'] == 'online' and fuseki_status['status'] == 'online':
                    st.success("‚úÖ Tous les endpoints sont accessibles!")
                else:
                    st.warning("‚ö†Ô∏è Certains endpoints ne sont pas accessibles")
    
    with col2:
        st.subheader("Informations sur l'environnement")
        
        # Composant d'information syst√®me
        system_info = SystemInfoDisplay()
        
        if st.button("Afficher les informations syst√®me"):
            info = system_info.get_system_summary()
            for key, value in info.items():
                st.write(f"**{key}:** {value}")
    
    st.subheader("S√©lection des requ√™tes")
    
    # R√©cup√©ration du catalogue de requ√™tes
    query_catalog = SPARQLQueryCatalog()
    all_queries = query_catalog.get_queries_by_type(sidebar_config["dataset_choice"])
    
    # Filtrage des requ√™tes selon la s√©lection
    selected_queries = filter_queries_by_selection(all_queries, sidebar_config["query_types"])
    
    # Affichage des requ√™tes s√©lectionn√©es
    if selected_queries:
        st.write(f"**{len(selected_queries)} requ√™tes s√©lectionn√©es:**")
        
        # Affichage en accord√©on
        for query_name, query in selected_queries.items():
            with st.expander(f"üìù {query_name}"):
                st.code(query, language="sparql")
                
                # Estimation de complexit√©
                complexity = query_catalog.get_query_complexity_estimate(query)
                col1, col2, col3 = st.columns(3)
                with col1:
                    st.metric("Complexit√©", complexity["level"])
                with col2:
                    st.metric("Score", complexity["score"])
                with col3:
                    st.metric("Temps estim√©", complexity["estimated_execution_time"])
    else:
        st.warning("Aucune requ√™te s√©lectionn√©e. Veuillez choisir au moins un type de requ√™te dans la barre lat√©rale.")
    
    # Requ√™te personnalis√©e
    st.subheader("Requ√™te personnalis√©e")
    
    col1, col2 = st.columns([3, 1])
    
    with col1:
        custom_query_name = st.text_input(
            "Nom de la requ√™te personnalis√©e", 
            value="Ma requ√™te personnalis√©e"
        )
        
        custom_query = st.text_area(
            "Entrez votre requ√™te SPARQL personnalis√©e",
            value="""SELECT ?s ?p ?o
WHERE {
    ?s ?p ?o .
}
LIMIT 10""",
            height=150
        )
    
    with col2:
        st.write("**Validation:**")
        if custom_query.strip():
            validation = query_catalog.validate_query(custom_query)
            if validation["valid"]:
                st.success("‚úÖ Syntaxe valide")
                complexity = query_catalog.get_query_complexity_estimate(custom_query)
                st.info(f"Complexit√©: {complexity['level']}")
            else:
                st.error(f"‚ùå {validation['error']}")
    
    include_custom = st.checkbox("Inclure la requ√™te personnalis√©e", value=False)
    
    if include_custom and custom_query.strip():
        selected_queries[custom_query_name] = custom_query
    
    # R√©sum√© de la configuration
    st.subheader("R√©sum√© de la configuration")
    
    config_summary = f"""
    **Endpoints:**
    - Virtuoso: `{sidebar_config["virtuoso_endpoint"]}`
    - Jena Fuseki: `{sidebar_config["fuseki_endpoint"]}`
    
    **Jeu de donn√©es:** {sidebar_config["dataset_choice"]}
    
    **Param√®tres de test:**
    - It√©rations: {sidebar_config["num_iterations"]}
    - √âchauffement: {sidebar_config["warmup_iterations"]}
    - Concurrence: {sidebar_config["concurrent_queries"]}
    - Timeout: {sidebar_config["query_timeout"]}s
    
    **Requ√™tes s√©lectionn√©es:** {len(selected_queries)}
    """
    
    st.markdown(config_summary)
    
    # Bouton d'ex√©cution des tests
    if st.button("üöÄ Ex√©cuter les tests", type="primary", use_container_width=True):
        if not selected_queries:
            st.error("‚ùå Veuillez s√©lectionner au moins une requ√™te √† tester.")
        else:
            execute_tests(selected_queries, sidebar_config)

def execute_tests(selected_queries: Dict[str, str], config: Dict[str, Any]):
    """
    Ex√©cute les tests de performance
    
    Args:
        selected_queries: Dictionnaire des requ√™tes √† tester
        config: Configuration des tests
    """
    try:
        # Initialisation du testeur
        tester = SPARQLPerformanceTester(
            config["virtuoso_endpoint"],
            config["fuseki_endpoint"]
        )
        
        # Configuration du timeout
        tester.executor.set_timeout(config["query_timeout"])
        
        # Cr√©ation des barres de progression
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        # Conteneur pour les r√©sultats interm√©diaires
        results_container = st.empty()
        
        # Stockage des r√©sultats
        results_df = pd.DataFrame()
        total_queries = len(selected_queries)
        
        log_message(f"D√©but des tests: {total_queries} requ√™tes")
        
        # Ex√©cution des tests pour chaque requ√™te
        for i, (query_name, query) in enumerate(selected_queries.items()):
            progress = i / total_queries
            progress_bar.progress(progress)
            status_text.text(f"üîÑ Test en cours: {query_name}")
            
            try:
                # Phase d'√©chauffement
                if config["warmup_iterations"] > 0:
                    status_text.text(f"üî• √âchauffement: {query_name}")
                    tester.run_benchmark(
                        query_name, 
                        query, 
                        config["warmup_iterations"], 
                        config["warmup_iterations"], 
                        is_warmup=True
                    )
                
                # Ex√©cution principale
                status_text.text(f"‚ö° Ex√©cution: {query_name}")
                query_results = tester.run_benchmark(
                    query_name, 
                    query, 
                    config["num_iterations"], 
                    config["warmup_iterations"]
                )
                
                if query_results is not None and not query_results.empty:
                    results_df = pd.concat([results_df, query_results], ignore_index=True)
                
                # Ex√©cution concurrente si demand√©e
                if config["concurrent_queries"] > 1:
                    status_text.text(f"üîÄ Test concurrent: {query_name}")
                    concurrent_results = tester.run_concurrent_benchmark(
                        query_name, 
                        query, 
                        config["num_iterations"], 
                        config["concurrent_queries"]
                    )
                    
                    if concurrent_results is not None and not concurrent_results.empty:
                        results_df = pd.concat([results_df, concurrent_results], ignore_index=True)
                
                # Affichage des r√©sultats interm√©diaires
                if not results_df.empty:
                    with results_container.container():
                        st.write(f"**R√©sultats interm√©diaires** ({i+1}/{total_queries} requ√™tes trait√©es)")
                        
                        # Statistiques rapides
                        latest_results = results_df[results_df['query_name'].str.contains(query_name)]
                        if not latest_results.empty:
                            avg_time = latest_results['execution_time'].mean()
                            success_rate = latest_results['success'].mean() * 100
                            
                            col1, col2, col3 = st.columns(3)
                            with col1:
                                st.metric("Temps moyen", f"{avg_time:.3f}s")
                            with col2:
                                st.metric("Taux de succ√®s", f"{success_rate:.1f}%")
                            with col3:
                                st.metric("Ex√©cutions", len(latest_results))
                
            except Exception as e:
                log_message(f"Erreur lors du test de {query_name}: {str(e)}", "error")
                st.error(f"‚ùå Erreur lors du test de {query_name}: {str(e)}")
        
        # Finalisation
        progress_bar.progress(1.0)
        status_text.text("‚úÖ Tests termin√©s !")
        
        if not results_df.empty:
            # Sauvegarde des r√©sultats
            save_test_results(results_df, config)
            
            # Affichage du r√©sum√© final
            st.success(f"üéâ Tests termin√©s avec succ√®s!")
            
            col1, col2, col3, col4 = st.columns(4)
            
            with col1:
                st.metric("Requ√™tes test√©es", total_queries)
            with col2:
                st.metric("Ex√©cutions totales", len(results_df))
            with col3:
                avg_time = results_df['execution_time'].mean()
                st.metric("Temps moyen", f"{avg_time:.3f}s")
            with col4:
                success_rate = results_df['success'].mean() * 100
                st.metric("Taux de succ√®s global", f"{success_rate:.1f}%")
            
            # Aper√ßu des r√©sultats
            st.subheader("Aper√ßu des r√©sultats")
            
            # Comparaison rapide par moteur
            engine_comparison = results_df.groupby('engine').agg({
                'execution_time': 'mean',
                'success': 'mean',
                'cpu_usage': 'mean',
                'memory_usage': 'mean'
            }).round(4)
            
            st.dataframe(engine_comparison, use_container_width=True)
            
            st.info("üìä Consultez l'onglet 'R√©sultats' pour une analyse d√©taill√©e et l'onglet 'Visualisation' pour les graphiques.")
            
        else:
            st.error("‚ùå Aucun r√©sultat g√©n√©r√©. V√©rifiez la connectivit√© et les requ√™tes.")
            
    except Exception as e:
        log_message(f"Erreur g√©n√©rale lors de l'ex√©cution des tests: {str(e)}", "error")
        st.error(f"‚ùå Erreur lors de l'ex√©cution des tests: {str(e)}")
        
        # Suggestions de d√©pannage
        with st.expander("üí° Suggestions de d√©pannage"):
            st.markdown("""
            **Probl√®mes courants et solutions:**
            
            1. **Endpoints non accessibles:**
               - V√©rifiez que Virtuoso et Fuseki sont d√©marr√©s
               - Testez la connectivit√© manuellement
               
            2. **Timeout des requ√™tes:**
               - Augmentez le timeout dans la barre lat√©rale
               - Utilisez des requ√™tes plus simples pour commencer
               
            3. **Erreurs de m√©moire:**
               - R√©duisez le nombre d'it√©rations
               - Diminuez le niveau de concurrence
               
            4. **Erreurs de syntaxe SPARQL:**
               - V√©rifiez vos requ√™tes personnalis√©es
               - Utilisez les requ√™tes pr√©d√©finies pour commencer
            """)
    
    finally:
        # Nettoyage
        progress_bar.empty()
        status_text.empty()